{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x12025c330>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x12025c330>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "@author: Jay Mehta\n",
    "\n",
    "Based on the work of Maziar Raissi\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import sys\n",
    "# Include the path that contains a number of files that have txt files containing solutions to the Burger's problem.\n",
    "sys.path.insert(0,'../../Utilities/')\n",
    "\n",
    "\n",
    "# Import required modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "from scipy.interpolate import griddata\n",
    "from pyDOE import lhs\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import time\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "\n",
    "np.random.seed(1234)\n",
    "torch.manual_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhysicsInformedNN:\n",
    "    # Initialize the class\n",
    "    \"\"\"\n",
    "    This class defined the Physics Informed Neural Network. The class is first initialized by the __init__ function. Additional functions related to the class are also defined subsequently.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X_u, u, X_f, layers, lb, ub, nu, epochs):\n",
    "\n",
    "        # Defining the lower and upper bound of the domain.\n",
    "        self.lb = lb\n",
    "        self.ub = ub\n",
    "\n",
    "        # Define the initial conditions for X and t\n",
    "        self.x_u = X_u[:,0:1]\n",
    "        self.t_u = X_u[:,1:2]\n",
    "\n",
    "        self.x_u_tf = self.x_u\n",
    "        self.t_u_tf = self.t_u\n",
    "\n",
    "        # Define the final conditions for X and t\n",
    "        self.x_f = X_f[:,0:1]\n",
    "        self.t_f = X_f[:,1:2]\n",
    "\n",
    "        self.x_f_tf = self.x_f\n",
    "        self.t_f_tf = self.t_f\n",
    "\n",
    "        # Declaring the field for the variable to be solved for\n",
    "        self.u = u\n",
    "        self.u_tf = u\n",
    "\n",
    "        # Declaring the number of layers in the Neural Network\n",
    "        self.layers = layers\n",
    "        # Defininf the diffusion constant in the problem (?)\n",
    "        self.nu = nu\n",
    "\n",
    "        # Create the structure of the neural network here, or build a function below to build the architecture and send the model here.\n",
    "\n",
    "        self.model = self.neural_net(layers)\n",
    "\n",
    "        # Define the initialize_NN function to obtain the initial weights and biases for the network.\n",
    "        self.model.apply(self.initialize_NN)\n",
    "\n",
    "        # Select the optimization method for the network. Currently, it is just a placeholder.\n",
    "\n",
    "        self.optimizer = torch.optim.SGD(self.model.parameters(), lr = 0.01)\n",
    "\n",
    "        for epoch in range(0,epochs):\n",
    "            u_pred = self.net_u(torch.from_numpy(self.x_u_tf), torch.from_numpy(self.t_u_tf))\n",
    "            f_pred = self.net_f(self.x_f_tf, self.t_f_tf)\n",
    "            loss = self.calc_loss(u_pred, self.u_tf, f_pred)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        # train(model,epochs,self.x_u_tf,self.t_u_tf,self.x_f_tf,self.t_f_tf,self.u_tf)\n",
    "\n",
    "    def neural_net(self, layers):\n",
    "        \"\"\"\n",
    "        A function to build the neural network of the required size using the weights and biases provided. Instead of doing this, can we use a simple constructor method and initalize them post the construction? That would be sensible and faster.\n",
    "        \"\"\"\n",
    "        model = nn.Sequential()\n",
    "        for l in range(0, len(layers) - 1):\n",
    "            model.add_module(\"layer_\"+str(l), nn.Linear(layers[l],layers[l+1], bias=True))\n",
    "            model.add_module(\"tanh_\"+str(l), nn.Tanh())\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "    def initialize_NN(self, m):\n",
    "        \"\"\"\n",
    "        Initialize the neural network with the required layers, the weights and the biases. The input \"layers\" in an array that contains the number of nodes (neurons) in each layer.\n",
    "        \"\"\"\n",
    "\n",
    "        if type(m) == nn.Linear:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            # print(m.weight)\n",
    "\n",
    "\n",
    "    def net_u(self, x, t):\n",
    "        \"\"\"\n",
    "        Forward pass through the network to obtain the U field.\n",
    "        \"\"\"\n",
    "\n",
    "        u = self.model(torch.cat((x,t),1).float())\n",
    "        return u\n",
    "\n",
    "\n",
    "    def net_f(self, x, t):\n",
    "        u = net_u(self.model, x, t)\n",
    "        u_x = torch.autograd.grad(u, x, grad_outputs = torch.tensor([[1.0],[1.0]]), create_graph = True)\n",
    "        u_xx = torch.autograd.grad(u_x, x, grad_outputs = torch.tensor([[1.0],[1.0]]), create_graph = True)\n",
    "        u = net_u(self.model, x, t)\n",
    "        u_t = torch.autograd.grad(u, t, create_graph = True)\n",
    "\n",
    "        f = u_t[0] + u * u_x[0] - self.nu * u_xx[0]\n",
    "\n",
    "        return f\n",
    "\n",
    "    def calc_loss(self, u_pred, u_tf, f_pred):\n",
    "        losses = torch.mean(torch.mul(u_pred - u_tf, u_pred - u_tf)) + torch.mean(torch.mul(f_pred, f_pred))\n",
    "\n",
    "        return losses\n",
    "\n",
    "    def train(self, model, epochs, x_u_tf, t_u_tf, x_f_tf, t_f_tf, u_tf):\n",
    "\n",
    "        for epoch in range(0,epochs):\n",
    "            # Now, one can perform a forward pass through the network to predict the value of u and f for various locations of x and at various times t. The function to call here is net_u and net_f.\n",
    "\n",
    "            # Here it is crucial to remember to provide x and t as columns and not as rows. Concatenation in the prediction step will fail otherwise.\n",
    "\n",
    "            u_pred = net_u(x_u_tf, t_u_tf)\n",
    "            f_pred = net_f(x_f_tf, t_f_tf)\n",
    "\n",
    "            # Now, we can define the loss of the network. The loss here is broken into two components: one is the loss due to miscalculating the predicted value of u, the other is for not satisfying the physical governing equation in f which must be equal to 0 at all times and all locations (strong form).\n",
    "\n",
    "            loss = calc_loss(u_pred, u_tf, f_pred)\n",
    "\n",
    "            # Calculate the gradients using the backward() method.\n",
    "\n",
    "            loss.backward() # Here, a tensor may need to be passed so that the gradients can be calculated.\n",
    "\n",
    "            # Optimize the parameters through the optimization step and the learning rate.\n",
    "            optimizer.step()\n",
    "\n",
    "            # Repeat the prediction, calculation of losses, and optimization a number of times to optimize the network.\n",
    "\n",
    "\n",
    "\n",
    "# layers = [2, 20, 20, 20, 20, 20, 20, 20, 20, 1]\n",
    "# model = neural_net(layers)\n",
    "# model.apply(initialize_NN)\n",
    "# model(torch.tensor([1.1,0.5])) # This is how you feed the network forward. Use model(x) where x has two inputs for the location and time.\n",
    "# x = torch.tensor([[1.1],[1.2]],requires_grad = True)\n",
    "# t = torch.tensor([[0.5],[0.5]],requires_grad = True)\n",
    "\n",
    "# u = model(torch.cat((x,t),1))\n",
    "# # print(torch.cat((x,t),1))\n",
    "# u.backward(torch.tensor([[1.0],[1.0]]))\n",
    "# # print(x.grad.data)\n",
    "# u_x = torch.autograd.grad(u,t,grad_outputs = torch.tensor([[1.0],[1.0]]),create_graph = True)\n",
    "\n",
    "# y = torch.tensor([1.],requires_grad = True)\n",
    "# x = torch.tensor([10.],requires_grad = True)\n",
    "# y2 = torch.cat((x,y))\n",
    "# print(y2)\n",
    "# A = torch.tensor([[2.,3.],[4.,5.]],requires_grad = True)\n",
    "# loss = (torch.mul(y2,y2)).sum()\n",
    "# print(torch.autograd.grad(loss,x))\n",
    "# print(torch.autograd.grad(loss,t))\n",
    "\n",
    "\n",
    "# u = net_u(model, x, t)\n",
    "# print(u)\n",
    "# u_x = torch.autograd.grad(u, x, create_graph = True)\n",
    "# u_xx = torch.autograd.grad(u, x, create_graph = True)\n",
    "# u = net_u(model, x, t)\n",
    "# u_t = torch.autograd.grad(u,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nu = 0.01/np.pi\n",
    "noise = 0.0\n",
    "\n",
    "N_u = 100\n",
    "N_f = 10000\n",
    "\n",
    "# Layer Map\n",
    "\n",
    "layers = [2, 20, 20, 20, 20, 20, 20, 20, 20, 1]\n",
    "\n",
    "data = scipy.io.loadmat('../../appendix/Data/burgers_shock.mat')\n",
    "\n",
    "t = data['t'].flatten()[:,None]\n",
    "x = data['x'].flatten()[:,None]\n",
    "Exact = np.real(data['usol']).T\n",
    "\n",
    "X, T = np.meshgrid(x,t)\n",
    "X_star = np.hstack((X.flatten()[:,None],T.flatten()[:,None]))\n",
    "u_star = Exact.flatten()[:,None]\n",
    "\n",
    "# Doman bounds\n",
    "lb = X_star.min(0)\n",
    "ub = X_star.max(0)\n",
    "\n",
    "xx1 = np.hstack((X[0:1,:].T, T[0:1,:].T))\n",
    "uu1 = Exact[0:1,:].T\n",
    "xx2 = np.hstack((X[:,0:1], T[:,0:1]))\n",
    "uu2 = Exact[:,0:1]\n",
    "xx3 = np.hstack((X[:,-1:], T[:,-1:]))\n",
    "uu3 = Exact[:,-1:]\n",
    "\n",
    "X_u_train = np.vstack([xx1, xx2, xx3])\n",
    "X_f_train = lb + (ub-lb)*lhs(2, N_f)\n",
    "X_f_train = np.vstack((X_f_train, X_u_train))\n",
    "u_train = np.vstack([uu1, uu2, uu3])\n",
    "\n",
    "idx = np.random.choice(X_u_train.shape[0], N_u, replace=False)\n",
    "X_u_train = X_u_train[idx, :]\n",
    "u_train = u_train[idx,:]\n",
    "\n",
    "# model = PhysicsInformedNN(X_u_train,u_train,X_f_train,layers,lb,ub,nu,5)\n",
    "X_u_train = torch.from_numpy(X_u_train)\n",
    "X_u_train.requires_grad = True\n",
    "u_train = torch.from_numpy(u_train)\n",
    "u_train.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_u = X_u_train[:,0:1]\n",
    "t_u = X_u_train[:,1:2]\n",
    "model = nn.Sequential()\n",
    "for l in range(0, len(layers) - 1):\n",
    "    model.add_module(\"layer_\"+str(l), nn.Linear(layers[l],layers[l+1], bias=True))\n",
    "    model.add_module(\"tanh_\"+str(l), nn.Tanh())\n",
    "    \n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr = 0.01, alpha = 0.9, momentum = 0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "for epoch in range(0,1000):\n",
    "\n",
    "    u_pred = model(torch.cat((x_u,t_u),1).float())\n",
    "    u_x = torch.autograd.grad(u_pred,x_u,grad_outputs = torch.ones([len(x_u),1],dtype = torch.float),create_graph=True)\n",
    "    u_xx = torch.autograd.grad(u_x,x_u,grad_outputs = torch.ones([len(x_u),1],dtype = torch.float),create_graph=True)\n",
    "    u_t = torch.autograd.grad(u_pred,t_u,grad_outputs = torch.ones([len(t_u),1],dtype = torch.float),create_graph=True)\n",
    "    f = u_t[0] + u_pred * u_x[0] - nu * u_xx[0]\n",
    "\n",
    "    loss = torch.mean(torch.mul(u_pred - u_train, u_pred - u_train)) + torch.mean(torch.mul(f,f))\n",
    "    losses.append(loss.detach().tolist())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
